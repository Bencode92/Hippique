import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import os
import traceback
from datetime import datetime, timedelta

class ScraperCoursesFG:
    def __init__(self):
        self.base_url = "https://www.france-galop.com"
        self.courses_aujourdhui_url = f"{self.base_url}/fr/courses/aujourdhui"
        self.output_dir = "data/courses"
        os.makedirs(self.output_dir, exist_ok=True)
        
    def get_driver(self):
        """Initialise et retourne un driver Selenium"""
        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        # Ajout d'un User-Agent plus r√©aliste
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36")
        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    def wait_and_get_html(self, driver, by, value, timeout=15):
        """Attend l'apparition d'un √©l√©ment et retourne le HTML"""
        try:
            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))
            return driver.page_source
        except Exception as e:
            print(f"‚ö†Ô∏è Timeout en attendant l'√©l√©ment {by}={value}: {str(e)}")
            return driver.page_source
    
    def extract_participants_table(self, course_soup):
        """Extrait les participants depuis le tableau de partants de la page de course"""
        table = course_soup.find("table")
        if not table:
            return []

        headers = []
        thead = table.find("thead")
        if thead:
            headers = [th.get_text(strip=True) for th in thead.find_all("th")]
        else:
            # Si pas de thead, prendre la premi√®re ligne comme en-t√™tes
            first_row = table.find("tr")
            if first_row:
                headers = [th.get_text(strip=True) for th in first_row.find_all("th") or first_row.find_all("td")]
        
        if not headers:
            print("‚ö†Ô∏è Aucun en-t√™te trouv√© dans le tableau des partants")
            return []
            
        print(f"üìä En-t√™tes trouv√©s: {headers}")

        body_rows = table.find("tbody").find_all("tr") if table.find("tbody") else table.find_all("tr")[1:] if len(table.find_all("tr")) > 1 else []
        participants = []
        
        print(f"üìã Trouv√© {len(body_rows)} lignes de participants")

        for row in body_rows:
            cells = row.find_all("td")
            if len(cells) < 3:  # Un participant valide doit avoir au moins quelques cellules
                continue

            participant = {}
            for i, cell in enumerate(cells):
                if i >= len(headers):
                    key = f"column_{i+1}"
                else:
                    key = headers[i].lower().replace(" ", "_").replace(".", "").replace("/", "_")
                
                value = cell.get_text(strip=True)
                participant[key] = value

                # Cherche lien
                link = cell.find("a")
                if link and link.get("href"):
                    href = link["href"]
                    if not href.startswith("http"):
                        if not href.startswith("/"):
                            href = "/" + href
                        href = self.base_url + href
                    participant[f"{key}_url"] = href

            # Ne conserver que les participants avec des donn√©es significatives
            if participant:
                participants.append(participant)

        return participants
    
    def extract_reunion_type(self, soup):
        """Extrait le type de r√©union (Plat/Obstacle) depuis la page de r√©union"""
        # Chercher les textes pouvant indiquer le type
        plat_indicator = soup.find(string=lambda text: text and "Plat :" in text)
        obstacle_indicator = soup.find(string=lambda text: text and "Obstacle :" in text)
        
        if plat_indicator:
            # Extraire le nombre de courses de plat si disponible
            plat_count = plat_indicator.strip().split(":")[1].strip() if ":" in plat_indicator else ""
            return {"type": "Plat", "count": plat_count}
        elif obstacle_indicator:
            # Extraire le nombre de courses d'obstacle si disponible
            obstacle_count = obstacle_indicator.strip().split(":")[1].strip() if ":" in obstacle_indicator else ""
            return {"type": "Obstacle", "count": obstacle_count}
        else:
            # Recherche alternative
            for element in soup.select(".discipline, [class*='discipline'], .type, [class*='type'], span"):
                text = element.get_text(strip=True).lower()
                if "plat" in text:
                    return {"type": "Plat", "count": ""}
                elif "obstacle" in text:
                    return {"type": "Obstacle", "count": ""}
        
        return {"type": "Ind√©termin√©", "count": ""}
    
    def get_real_hippodrome_name(self, soup, default_name="Hippodrome"):
        """Extrait le vrai nom de l'hippodrome depuis la page"""
        # Chercher le titre principal
        title = soup.select_one("h1.reunion-title, h1.title, h1")
        if title:
            text = title.get_text(strip=True)
            # V√©rifier si une date est pr√©sente dans le titre, la retirer si c'est le cas
            if " - " in text:
                # Souvent "DATE - NOM_HIPPODROME"
                parts = text.split(" - ")
                if len(parts) == 2:
                    # V√©rifier si la premi√®re partie ressemble √† une date
                    if any(month in parts[0].lower() for month in ["janvier", "f√©vrier", "mars", "avril", "mai", "juin", "juillet", "ao√ªt", "septembre", "octobre", "novembre", "d√©cembre"]):
                        return parts[1].strip()
                    # Ou si la deuxi√®me partie ressemble √† une date
                    if any(month in parts[1].lower() for month in ["janvier", "f√©vrier", "mars", "avril", "mai", "juin", "juillet", "ao√ªt", "septembre", "octobre", "novembre", "d√©cembre"]):
                        return parts[0].strip()
            return text
        
        # Chercher dans d'autres √©l√©ments
        for selector in [".hippodrome-name", ".location", "[class*='hippodrome']", "[class*='location']"]:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        # Chercher dans le titre de la page
        if soup.title:
            title_text = soup.title.get_text(strip=True)
            # Si le titre contient le nom de l'hippodrome
            for word in title_text.split(" - "):
                if word.upper() == word and len(word) > 3:  # Souvent les noms d'hippodromes sont en majuscules
                    return word
        
        # Chercher un √©l√©ment qui contient le mot "hippodrome"
        for element in soup.select("h2, h3, div.title, p"):
            text = element.get_text(strip=True)
            if "hippodrome" in text.lower():
                return text
        
        return default_name
    
    def get_links_from_aujourdhui_page(self, driver):
        """R√©cup√®re les liens des r√©unions pour aujourd'hui, toutes disciplines confondues"""
        print(f"üìÖ Acc√®s √† la page des courses du jour: {self.courses_aujourdhui_url}")
        driver.get(self.courses_aujourdhui_url)
        time.sleep(5)  # Attendre que la page se charge

        print("üîç Scraping des courses du jour (page 'aujourdhui')...")
        links = []

        # MODIFICATION: Changement du chemin pour les captures d'√©cran de d√©bogage
        screenshot_dir = os.path.join("data", "debug_captures")
        os.makedirs(screenshot_dir, exist_ok=True)
        screenshot_path = os.path.join(screenshot_dir, "aujourdhui_page.png")
        driver.save_screenshot(screenshot_path)
        print(f"üì∏ Capture d'√©cran sauvegard√©e: {screenshot_path}")
        
        # Sauvegardons le HTML pour le d√©bogage
        html = driver.page_source
        with open(os.path.join(screenshot_dir, "aujourdhui_page.html"), "w", encoding="utf-8") as f:
            f.write(html)

        soup = BeautifulSoup(html, "html.parser")
        
        # Extraire les blocs d'hippodromes
        hippodrome_cards = soup.select(".card, .reunion-card, [class*='hippodrome-card'], [class*='reunion']")
        
        if hippodrome_cards:
            print(f"üìä Trouv√© {len(hippodrome_cards)} cartes d'hippodromes")
            
            for card in hippodrome_cards:
                # Extraire le nom de l'hippodrome
                name_element = card.select_one("h2, h3, .title, [class*='title']")
                hippodrome_name = name_element.get_text(strip=True) if name_element else "Hippodrome"
                
                # D√©terminer le type (Plat/Obstacle)
                type_element = card.select_one(".discipline, [class*='discipline'], [class*='type']")
                type_text = type_element.get_text(strip=True) if type_element else ""
                reunion_type = "Plat" if "plat" in type_text.lower() else "Obstacle" if "obstacle" in type_text.lower() else "Ind√©termin√©"
                
                # Chercher les indicateurs de plat ou obstacle
                plat_element = card.find(string=lambda text: text and "Plat :" in text)
                obstacle_element = card.find(string=lambda text: text and "Obstacle :" in text)
                
                if plat_element:
                    reunion_type = "Plat"
                    # Extraire le nombre
                    try:
                        count = plat_element.strip().split(":")[1].strip()
                        reunion_type_count = count
                    except:
                        reunion_type_count = ""
                elif obstacle_element:
                    reunion_type = "Obstacle"
                    # Extraire le nombre
                    try:
                        count = obstacle_element.strip().split(":")[1].strip()
                        reunion_type_count = count
                    except:
                        reunion_type_count = ""
                else:
                    reunion_type_count = ""
                
                # Trouver l'URL
                link_element = card.select_one("a")
                if link_element and link_element.get("href"):
                    href = link_element.get("href")
                    url = self.base_url + href if href.startswith("/") else href
                    
                    links.append({
                        "hippodrome": hippodrome_name,
                        "url": url,
                        "type": reunion_type,
                        "count": reunion_type_count
                    })
                    print(f"‚úÖ Hippodrome trouv√©: {hippodrome_name} ({reunion_type}) - {url}")
        
        # Si aucun hippodrome n'est trouv√© avec la m√©thode des cartes, utiliser la m√©thode des liens
        if not links:
            # M√©thode 1: Cartes de r√©union standard
            cards = soup.select("a.reunion-card")
            print(f"üìä M√©thode 1: Trouv√© {len(cards)} cartes de r√©union")
            
            # M√©thode 2: Approche plus g√©n√©rale - liens de r√©union
            if not cards:
                cards = soup.select("a[href*='fiche-reunion']")
                print(f"üìä M√©thode 2: Trouv√© {len(cards)} liens de r√©union")
            
            # M√©thode 3: Chercher tous les liens qui pourraient √™tre des r√©unions
            if not cards:
                cards = soup.select("a[href*='reunion']")
                print(f"üìä M√©thode 3: Trouv√© {len(cards)} liens contenant 'reunion'")
            
            # M√©thode 4: Chercher dans les tableaux
            if not cards:
                tables = soup.select("table")
                for table in tables:
                    links_in_table = table.select("a")
                    if links_in_table:
                        print(f"üìä M√©thode 4: Trouv√© {len(links_in_table)} liens dans un tableau")
                        cards.extend(links_in_table)
            
            # Traitement des liens trouv√©s
            for card in cards:
                href = card.get("href")
                if not href or href == "#":
                    continue
                    
                # Trouver le nom de l'hippodrome
                lieu = card.select_one("h2, .title, .hippodrome, .reunion-title")
                if not lieu:
                    # Si pas de titre standard, utiliser le texte du lien lui-m√™me
                    lieu_text = card.get_text(strip=True)
                    hippodrome = lieu_text if lieu_text else "Hippodrome"
                else:
                    hippodrome = lieu.text.strip()
                
                # V√©rifier si c'est une course de Plat ou d'Obstacle
                type_course_element = card.select_one(".discipline, [class*='discipline'], .type-course, [class*='type']")
                type_course = type_course_element.text.strip().lower() if type_course_element else ""
                
                reunion_type = "Plat" if "plat" in type_course.lower() else "Obstacle" if "obstacle" in type_course.lower() else "Ind√©termin√©"
                
                url_reunion = self.base_url + href if href.startswith("/") else href
                print(f"‚úÖ R√©union trouv√©e : {hippodrome} ({reunion_type}) - {url_reunion}")
                links.append({
                    "hippodrome": hippodrome,
                    "url": url_reunion,
                    "type": reunion_type,
                    "count": ""
                })

        print(f"üèÅ Total: {len(links)} r√©unions trouv√©es aujourd'hui")
        return links
    
    def extract_course_details(self, driver, course_url, hippodrome, reunion_type="Ind√©termin√©", is_course=False):
        """Extrait les d√©tails de toutes les courses d'un hippodrome"""
        print(f"üìç Scraping des courses √† {hippodrome}...")
        
        driver.get(course_url)
        time.sleep(5)  # Augmentation du d√©lai
        
        # V√©rifier si la page a √©t√© correctement charg√©e
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        
        # Essayer d'extraire le vrai nom de l'hippodrome depuis la page
        real_hippodrome_name = self.get_real_hippodrome_name(soup, hippodrome)
        if real_hippodrome_name and real_hippodrome_name != hippodrome:
            print(f"üèá Nom r√©el de l'hippodrome trouv√©: {real_hippodrome_name}")
            hippodrome = real_hippodrome_name
        
        # Extraire le type de r√©union (Plat/Obstacle) si pas d√©j√† d√©termin√©
        if reunion_type == "Ind√©termin√©":
            reunion_info = self.extract_reunion_type(soup)
            reunion_type = reunion_info["type"]
            print(f"üèá Type de r√©union d√©tect√©: {reunion_type}")
        
        courses_data = {
            "hippodrome": hippodrome,
            "type_reunion": reunion_type,
            "date_extraction": datetime.now().isoformat(),
            "url_source": course_url,
            "courses": []
        }
        
        try:
            # MODIFICATION: Changement du chemin pour les captures d'√©cran de d√©bogage
            screenshot_dir = os.path.join("data", "debug_captures")
            os.makedirs(screenshot_dir, exist_ok=True)
            
            # Cr√©er un nom de fichier s√©curis√© pour l'hippodrome
            safe_hippodrome = hippodrome.lower().replace(' ', '_').replace('/', '-').replace('\\', '-').replace(':', '-')
            
            screenshot_path = os.path.join(screenshot_dir, f"{safe_hippodrome}_reunion.png")
            driver.save_screenshot(screenshot_path)
            print(f"üì∏ Capture d'√©cran sauvegard√©e: {screenshot_path}")
            
            # Sauvegardons le HTML pour le d√©bogage
            with open(os.path.join(screenshot_dir, f"{safe_hippodrome}_reunion.html"), "w", encoding="utf-8") as f:
                f.write(html)
            
            # NOUVELLE APPROCHE: Trouver le tableau des courses sur la page de r√©union
            course_table = soup.select_one("table")
            if not course_table:
                print("‚ö†Ô∏è Aucun tableau de courses trouv√© sur la page de r√©union")
                
                # Si pas de tableau, essayer de trouver des liens directs vers les courses
                course_links = soup.select("a[href*='fiche-course']")
                if course_links:
                    print(f"üîÑ Alternative: trouv√© {len(course_links)} liens directs vers des courses")
                    
                    for index, link in enumerate(course_links):
                        course_name = link.get_text(strip=True)
                        href = link.get("href")
                        course_url = self.base_url + href if href.startswith("/") else href
                        
                        print(f"üîç Course {index+1}/{len(course_links)}: {course_name}")
                        
                        try:
                            # Ouvrir la page de la course pour extraire les participants
                            driver.get(course_url)
                            time.sleep(3)
                            
                            course_html = driver.page_source
                            course_soup = BeautifulSoup(course_html, "html.parser")
                            
                            # Extraire les d√©tails de la course
                            horaire = ""
                            horaire_element = course_soup.select_one(".horaire, .time, [class*='horaire']")
                            if horaire_element:
                                horaire = horaire_element.get_text(strip=True)
                            
                            numero = f"{index+1}"
                            numero_element = course_soup.select_one(".numero, .course-number, [class*='numero']")
                            if numero_element:
                                numero = numero_element.get_text(strip=True)
                            
                            # D√©terminer le type de course (plat/obstacle)
                            course_type = "Ind√©termin√©"
                            type_elements = course_soup.select(".discipline, [class*='discipline'], [class*='type']")
                            for elem in type_elements:
                                text = elem.get_text(strip=True).lower()
                                if "plat" in text:
                                    course_type = "Plat"
                                    break
                                elif "obstacle" in text:
                                    course_type = "Obstacle"
                                    break
                            
                            # Si pas trouv√©, utiliser le type de la r√©union
                            if course_type == "Ind√©termin√©":
                                course_type = reunion_type
                            
                            # Extraire la distance
                            distance = ""
                            distance_element = course_soup.select_one(".distance")
                            if distance_element:
                                distance = distance_element.get_text(strip=True)
                            
                            course_data = {
                                "nom": course_name,
                                "horaire": horaire,
                                "numero": numero,
                                "type": course_type,
                                "distance": distance,  # Ajout de la distance ici
                                "url": course_url,
                                "participants": []
                            }
                            
                            # Extraire les participants
                            participants = self.extract_participants_table(course_soup)
                            if participants:
                                course_data["participants"] = participants
                                print(f"‚úÖ Extrait {len(participants)} participants pour {course_name}")
                                courses_data["courses"].append(course_data)
                                print(f"‚úÖ Course ajout√©e: {course_name}")
                            else:
                                print(f"‚ö†Ô∏è Aucun participant trouv√© pour {course_name}")
                        
                        except Exception as e:
                            print(f"‚ùå Erreur lors du traitement de la course {course_name}: {str(e)}")
                            traceback.print_exc()
                    
                    return courses_data
                    
                return courses_data
                
            # Extraire les lignes du tableau (chaque ligne = une course)
            course_rows = course_table.select("tbody tr")
            if not course_rows:
                course_rows = course_table.select("tr")[1:] if len(course_table.select("tr")) > 1 else []
                
            print(f"üîé Trouv√© {len(course_rows)} courses dans la r√©union")
            
            # Parcourir chaque ligne (course) dans le tableau
            for index, row in enumerate(course_rows):
                # Extraire les informations basiques de la course depuis le tableau
                cells = row.select("td")
                if len(cells) < 2:  # V√©rifier si la ligne a suffisamment de cellules
                    continue
                    
                # Extraire l'horaire (premi√®re cellule g√©n√©ralement)
                horaire = cells[0].get_text(strip=True)
                
                # Extraire le num√©ro/ordre de la course
                numero = cells[1].get_text(strip=True) if len(cells) > 1 else ""
                
                # Extraire le nom de la course - chercher un lien dans une des cellules
                course_name = ""
                course_link = None
                
                for cell in cells:
                    link = cell.select_one("a")
                    if link:
                        course_name = link.get_text(strip=True)
                        href = link.get("href")
                        if href:
                            course_link = self.base_url + href if href.startswith("/") else href
                        break
                
                # Si on n'a pas trouv√© de lien, prendre le texte de la 3√®me cellule comme nom
                if not course_name and len(cells) > 2:
                    course_name = cells[2].get_text(strip=True)
                    
                # Si toujours pas de nom, g√©n√©rer un nom g√©n√©rique
                if not course_name:
                    course_name = f"Course {index+1}"
                
                # Extraire la distance si disponible
                distance = ""
                # Chercher une cellule avec la classe "distance" ou qui contient "m" (m√®tres)
                for i, cell in enumerate(cells):
                    if "distance" in cell.get("class", []) or (cell.get_text(strip=True).endswith("m") and cell.get_text(strip=True).replace("m", "").isdigit()):
                        distance = cell.get_text(strip=True)
                        break
                    # Si pas de classe sp√©cifique, chercher dans les valeurs num√©riques suivies de "m"
                    if cell.get_text(strip=True).isdigit() and i+1 < len(cells) and cells[i+1].get_text(strip=True).lower() == "m":
                        distance = cell.get_text(strip=True) + "m"
                        break
                    # En dernier recours, chercher dans les colonnes o√π pourrait figurer la distance
                    if i >= 3 and i <= 5 and cell.get_text(strip=True).isdigit():  # Les colonnes 3, 4, 5 sont souvent des distances
                        distance = cell.get_text(strip=True)
                        break
                
                print(f"‚úÖ Course identifi√©e: {horaire} - {numero} - {course_name} - Distance: {distance}")
                
                # D√©terminer le type de course
                course_type = reunion_type  # Par d√©faut, m√™me type que la r√©union
                
                # Cr√©er l'objet de donn√©es de base pour cette course
                course_data = {
                    "nom": course_name,
                    "horaire": horaire,
                    "numero": numero,
                    "type": course_type,
                    "distance": distance,  # Ajout de la distance ici
                    "url": course_link,
                    "participants": []
                }
                
                # Si nous avons un lien vers la page d√©taill√©e et que nous voulons les participants
                if course_link:
                    print(f"  ‚è≥ Navigation vers la page de d√©tails: {course_link}")
                    
                    try:
                        # Ouvrir la page de la course pour extraire les participants
                        driver.get(course_link)
                        time.sleep(3)
                        
                        # Capture d'√©cran pour d√©bogage
                        course_screenshot_path = os.path.join(screenshot_dir, f"{safe_hippodrome}_course_{index+1}.png")
                        driver.save_screenshot(course_screenshot_path)
                        
                        course_html = driver.page_source
                        course_soup = BeautifulSoup(course_html, "html.parser")
                        
                        # Essayer de d√©terminer le type de course plus pr√©cis√©ment
                        type_elements = course_soup.select(".discipline, [class*='discipline'], [class*='type']")
                        for elem in type_elements:
                            text = elem.get_text(strip=True).lower()
                            if "plat" in text:
                                course_data["type"] = "Plat"
                                break
                            elif "obstacle" in text:
                                course_data["type"] = "Obstacle"
                                break
                        
                        # V√©rifier si on peut trouver une distance plus pr√©cise sur la page d√©taill√©e
                        if not distance:
                            distance_element = course_soup.select_one(".distance, [class*='distance']")
                            if distance_element:
                                course_data["distance"] = distance_element.get_text(strip=True)
                        
                        # Extraire le tableau des participants
                        participants = self.extract_participants_table(course_soup)
                        
                        if participants:
                            course_data["participants"] = participants
                            print(f"  ‚úÖ Extrait {len(participants)} participants pour {course_name}")
                            # Ajouter cette course uniquement si elle a des participants
                            courses_data["courses"].append(course_data)
                        else:
                            print(f"  ‚ö†Ô∏è Aucun participant trouv√© pour {course_name}")
                            
                        # Revenir √† la page de r√©union pour continuer
                        driver.back()
                        time.sleep(2)
                    
                    except Exception as e:
                        print(f"‚ùå Erreur lors de l'acc√®s aux d√©tails de {course_name}: {str(e)}")
                        # Essayer de revenir √† la page de r√©union m√™me en cas d'erreur
                        try:
                            driver.get(course_url)
                            time.sleep(2)
                        except:
                            pass
                
            # Si pas de courses d√©tect√©es par le tableau, essayer une autre approche
            if not courses_data["courses"]:
                print("‚ö†Ô∏è Aucune course d√©tect√©e via le tableau, tentative d'approche alternative")
                
                # Approche alternative: chercher des cartes ou √©l√©ments contenant des infos de course
                course_elements = soup.select(".course-card, .race-card, [class*='course'], [class*='race']")
                if course_elements:
                    print(f"üîÑ Alternative: trouv√© {len(course_elements)} √©l√©ments de course")
                    
                    for index, elem in enumerate(course_elements):
                        # Extraire les infos disponibles
                        horaire = ""
                        horaire_elem = elem.select_one(".horaire, .time, [class*='time'], [class*='horaire']")
                        if horaire_elem:
                            horaire = horaire_elem.get_text(strip=True)
                            
                        course_name = f"Course {index+1}"
                        name_elem = elem.select_one("h2, h3, .title, [class*='title']")
                        if name_elem:
                            course_name = name_elem.get_text(strip=True)
                            
                        # Chercher un lien
                        course_link = None
                        link = elem.select_one("a")
                        if link and link.get("href"):
                            href = link.get("href")
                            course_link = self.base_url + href if href.startswith("/") else href
                            
                        # D√©terminer le type de course
                        course_type = reunion_type
                        type_elem = elem.select_one(".discipline, [class*='discipline'], [class*='type']")
                        if type_elem:
                            text = type_elem.get_text(strip=True).lower()
                            if "plat" in text:
                                course_type = "Plat"
                            elif "obstacle" in text:
                                course_type = "Obstacle"
                        
                        # Extraire la distance si disponible
                        distance = ""
                        distance_elem = elem.select_one(".distance, [class*='distance']")
                        if distance_elem:
                            distance = distance_elem.get_text(strip=True)
                            
                        course_data = {
                            "nom": course_name,
                            "horaire": horaire,
                            "numero": str(index+1),
                            "type": course_type,
                            "distance": distance,  # Ajout de la distance ici
                            "url": course_link,
                            "participants": []
                        }
                        
                        # Si on a un lien, visiter la page d√©taill√©e
                        if course_link:
                            try:
                                driver.get(course_link)
                                time.sleep(3)
                                
                                course_html = driver.page_source
                                course_soup = BeautifulSoup(course_html, "html.parser")
                                
                                # V√©rifier si on peut trouver une distance plus pr√©cise sur la page d√©taill√©e
                                if not distance:
                                    distance_element = course_soup.select_one(".distance, [class*='distance']")
                                    if distance_element:
                                        course_data["distance"] = distance_element.get_text(strip=True)
                                
                                participants = self.extract_participants_table(course_soup)
                                if participants:
                                    course_data["participants"] = participants
                                    print(f"‚úÖ Alternative: Extrait {len(participants)} participants pour {course_name}")
                                    # Ajouter cette course uniquement si elle a des participants
                                    courses_data["courses"].append(course_data)
                                else:
                                    print(f"‚ö†Ô∏è Aucun participant trouv√© pour {course_name}")
                                
                                driver.back()
                                time.sleep(2)
                                
                            except Exception as e:
                                print(f"‚ùå Erreur lors de l'acc√®s √† la course alternative {course_name}: {str(e)}")
                                try:
                                    driver.get(course_url)
                                    time.sleep(2)
                                except:
                                    pass
            
            return courses_data
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction des courses √† {hippodrome}: {str(e)}")
            traceback.print_exc()
            courses_data["error"] = str(e)
            return courses_data
    
    def save_json(self, data, filename):
        """Sauvegarde les donn√©es au format JSON"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ Donn√©es sauvegard√©es dans {filepath}")
    
    def enrich_existing_json_files(self):
        """Parcourt tous les fichiers JSON de course existants pour les enrichir avec les d√©tails"""
        files = [f for f in os.listdir(self.output_dir) if f.endswith(".json")]
        
        print(f"üîÑ Enrichissement de {len(files)} fichiers JSON existants...")
        
        for filename in files:
            filepath = os.path.join(self.output_dir, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)

            url = data.get("url_source")
            hippodrome = data.get("hippodrome", filename.replace(".json", ""))
            reunion_type = data.get("type_reunion", "Ind√©termin√©")
            
            if not url:
                print(f"‚ö†Ô∏è Pas d'URL source dans {filename}, fichier ignor√©.")
                continue
            
            print(f"üîç Re-scraping de {hippodrome} depuis {url}")
            driver = self.get_driver()
            try:
                enriched_data = self.extract_course_details(driver, url, hippodrome, reunion_type)
                if enriched_data.get("courses"):
                    self.save_json(enriched_data, filename)
                else:
                    print(f"‚ö†Ô∏è Aucune course avec participants trouv√©e pour {hippodrome}, fichier non mis √† jour")
            except Exception as e:
                print(f"‚ùå Erreur sur {filename}: {e}")
                traceback.print_exc()
            finally:
                driver.quit()
    
    def run_today_only(self):
        """Ex√©cute le scraper uniquement pour les courses du jour"""
        print("üìÜ Scraping uniquement des r√©unions d'aujourd'hui")
        driver = self.get_driver()

        try:
            courses_today = self.get_links_from_aujourdhui_page(driver)
            
            if not courses_today:
                print("‚ö†Ô∏è Aucune r√©union trouv√©e pour aujourd'hui")
                return

            # Dictionnaire pour stocker les r√©unions par nom d'hippodrome
            hippodromes_processed = {}

            for i, course in enumerate(courses_today):
                hippodrome_name = course["hippodrome"]
                reunion_type = course.get("type", "Ind√©termin√©")
                
                # √âviter les noms g√©n√©riques comme "Plus"
                if hippodrome_name.lower() in ["plus", "hippodrome", "hippodrome non identifi√©"]:
                    # Visiter la page pour essayer d'extraire le vrai nom
                    print(f"üîç Tentative d'extraction du vrai nom pour: {hippodrome_name}")
                    driver.get(course["url"])
                    time.sleep(3)
                    html = driver.page_source
                    soup = BeautifulSoup(html, "html.parser")
                    real_name = self.get_real_hippodrome_name(soup, hippodrome_name)
                    if real_name and real_name != hippodrome_name:
                        print(f"‚úÖ Vrai nom extrait: {real_name}")
                        hippodrome_name = real_name
                
                print(f"‚è≥ Traitement {i+1}/{len(courses_today)}: {hippodrome_name} ({reunion_type})")
                
                # Extraire les d√©tails des courses
                is_course = course.get("is_course", False)
                course_data = self.extract_course_details(driver, course["url"], hippodrome_name, reunion_type, is_course)
                
                # Ne sauvegarder que s'il y a des courses avec des participants
                if course_data.get("courses"):
                    # G√©n√©rer un nom de fichier bas√© sur l'hippodrome et la date
                    date_str = datetime.now().strftime("%Y-%m-%d")
                    safe_name = hippodrome_name.replace(" ", "_").replace("/", "-").replace("\\", "-").replace(":", "-").lower()
                    
                    # S'assurer que le nom de fichier est unique si plusieurs r√©unions portent le m√™me nom
                    count = hippodromes_processed.get(hippodrome_name, 0)
                    filename = f"{date_str}_{safe_name}.json" if count == 0 else f"{date_str}_{safe_name}_{count+1}.json"
                    hippodromes_processed[hippodrome_name] = count + 1
                    
                    # Sauvegarder les donn√©es
                    self.save_json(course_data, filename)
                    print(f"‚úÖ Donn√©es sauvegard√©es pour {hippodrome_name} avec {len(course_data['courses'])} courses")
                else:
                    print(f"‚ö†Ô∏è Aucune course avec participants trouv√©e pour {hippodrome_name}, fichier non sauvegard√©")
            
            print(f"üéâ Scraping termin√©! {len(courses_today)} hippodromes trait√©s.")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du scraping des courses du jour: {str(e)}")
            traceback.print_exc()
        finally:
            driver.quit()
    
    def direct_scrape_url(self, url, filename=None):
        """Scrape directement une URL sp√©cifique de course"""
        print(f"üîç Scraping direct de l'URL: {url}")
        
        driver = self.get_driver()
        try:
            # Visiter d'abord l'URL pour essayer d'extraire le vrai nom d'hippodrome
            driver.get(url)
            time.sleep(3)
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            hippodrome = self.get_real_hippodrome_name(soup, "Hippodrome_Direct")
            reunion_info = self.extract_reunion_type(soup)
            
            print(f"‚úÖ Nom d'hippodrome identifi√©: {hippodrome} ({reunion_info['type']})")
            
            # Extraire les d√©tails des courses
            course_data = self.extract_course_details(driver, url, hippodrome, reunion_info["type"])
            
            # Si un nom de fichier n'est pas fourni, en g√©n√©rer un
            if not filename:
                date_str = datetime.now().strftime("%Y-%m-%d")
                safe_name = hippodrome.replace(" ", "_").replace("/", "-").replace("\\", "-").replace(":", "-").lower()
                filename = f"{date_str}_{safe_name}_direct.json"
            
            # Sauvegarder les donn√©es seulement s'il y a des courses avec participants
            if course_data.get("courses"):
                self.save_json(course_data, filename)
                print(f"‚úÖ Donn√©es sauvegard√©es pour le scraping direct avec {len(course_data['courses'])} courses")
            else:
                print(f"‚ö†Ô∏è Aucune course avec participants trouv√©e pour le scraping direct, fichier non sauvegard√©")
            
            print(f"‚úÖ Scraping direct termin√© pour {url}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du scraping direct: {str(e)}")
            traceback.print_exc()
        finally:
            driver.quit()

if __name__ == "__main__":
    import os
    import sys
    
    # Obtenir les variables d'environnement (utile pour GitHub Actions)
    mode = os.environ.get("MODE", "today")  # "today", "enrich", "direct"
    direct_url = os.environ.get("URL", "")
    
    scraper = ScraperCoursesFG()
    
    # V√©rifier si on a des arguments en ligne de commande
    if len(sys.argv) > 1:
        # Si premier argument est une URL, faire un scraping direct
        if sys.argv[1].startswith("http"):
            direct_url = sys.argv[1]
            mode = "direct"
        # Sinon consid√©rer que c'est le mode
        else:
            mode = sys.argv[1]
    
    # Afficher le mode d'ex√©cution pour d√©bogage
    print(f"üöÄ Ex√©cution du scraper en mode: {mode}")
    
    # Mode selon l'environnement ou valeur par d√©faut
    if mode == "direct" and direct_url:
        # Scraping direct d'une URL
        scraper.direct_scrape_url(direct_url)
    elif mode == "enrich":
        # Enrichir les JSON avec les d√©tails internes
        scraper.enrich_existing_json_files()
    else:
        # Mode par d√©faut: scraper uniquement les courses du jour
        scraper.run_today_only()

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import os
import traceback
from datetime import datetime, timedelta

class ScraperCoursesFG:
    def __init__(self):
        self.base_url = "https://www.france-galop.com"
        self.courses_url = f"{self.base_url}/fr/courses/aujourd'hui"  # URL modifi√©e pour la page des courses du jour
        self.output_dir = "data/courses"
        os.makedirs(self.output_dir, exist_ok=True)
        
    def get_driver(self):
        """Initialise et retourne un driver Selenium"""
        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        # Ajout d'un User-Agent plus r√©aliste
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36")
        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    def wait_and_get_html(self, driver, by, value, timeout=15):
        """Attend l'apparition d'un √©l√©ment et retourne le HTML"""
        try:
            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))
            return driver.page_source
        except Exception as e:
            print(f"‚ö†Ô∏è Timeout en attendant l'√©l√©ment {by}={value}: {str(e)}")
            return driver.page_source
    
    def get_course_links(self, driver, filtre_type="Plat", jours=1):
        """R√©cup√®re les liens des courses selon le filtre et la p√©riode"""
        print(f"üîç Recherche des courses de {filtre_type} pour les {jours} prochains jours...")
        
        driver.get(self.courses_url)
        time.sleep(5)  # Augmentation du d√©lai pour s'assurer que la page est charg√©e
        
        # Liste pour stocker les liens des courses
        links_courses = []
        
        # Date d'aujourd'hui et p√©riode
        today = datetime.now()
        
        try:
            # V√©rifions d'abord si la page a bien charg√©
            print(f"üåê URL actuelle: {driver.current_url}")
            
            # Prenons une capture d'√©cran pour le d√©bogage
            screenshot_dir = os.path.join(self.output_dir, "debug")
            os.makedirs(screenshot_dir, exist_ok=True)
            screenshot_path = os.path.join(screenshot_dir, "courses_list.png")
            driver.save_screenshot(screenshot_path)
            print(f"üì∏ Capture d'√©cran sauvegard√©e: {screenshot_path}")
            
            html = driver.page_source
            
            # Sauvegardons le HTML pour le d√©bogage
            with open(os.path.join(screenshot_dir, "courses_list.html"), "w", encoding="utf-8") as f:
                f.write(html)
            
            soup = BeautifulSoup(html, "html.parser")
            
            # Nouveau code pour extraire les cartes des hippodromes
            cards = soup.select(".card-panel, [class*='card']")  # S√©lecteur CSS plus inclusif
            print(f"üé¥ Trouv√© {len(cards)} cartes d'hippodromes sur la page")
            
            for card in cards:
                # Chercher si cette carte contient une indication "Plat : X"
                card_text = card.get_text().strip()
                
                # V√©rifier si c'est une course de plat (la mention "Plat : X" doit appara√Ætre)
                plat_indicator = False
                if "Plat :" in card_text or "Plat:" in card_text:
                    plat_indicator = True
                    print(f"‚úÖ Trouv√© une carte pour courses de Plat: {card_text[:50]}...")
                else:
                    print(f"‚ùå Ignor√© une carte non-Plat: {card_text[:50]}...")
                    continue  # Passer √† la carte suivante si ce n'est pas une course de plat
                
                # Extraire le nom de l'hippodrome
                hippodrome_element = card.select_one("h3, h2, [class*='title'], strong, b")
                hippodrome = hippodrome_element.text.strip() if hippodrome_element else "Hippodrome inconnu"
                
                # Extraire l'URL
                link = card.select_one("a[href*='courses']")
                if link and link.get("href"):
                    href = link.get("href")
                    full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                    print(f"üèÅ Trouv√© course de Plat: {hippodrome} - {full_url}")
                    links_courses.append({"url": full_url, "hippodrome": hippodrome})
            
            # Si on ne trouve rien avec la nouvelle m√©thode, essayer l'ancienne approche
            if not links_courses:
                print("‚ö†Ô∏è Aucune course trouv√©e avec la m√©thode principale, essai de la m√©thode alternative...")
                
                # M√©thode alternative pour le cas o√π la structure de la page serait diff√©rente
                rows = soup.select("table tbody tr")
                print(f"üìã Trouv√© {len(rows)} lignes dans le tableau (m√©thode alternative)")
                
                for row in rows:
                    # Chercher un indicateur de type de course (Plat vs Obstacle)
                    row_text = row.get_text().strip()
                    if "Plat" in row_text and "Obstacle" not in row_text:
                        # Extraire le lien de la course
                        link_tag = row.select_one("td a")
                        if link_tag and link_tag.get("href"):
                            full_url = f"{self.base_url}{link_tag['href']}" if link_tag['href'].startswith('/') else link_tag['href']
                            hippodrome = link_tag.text.strip()
                            print(f"üèÅ Trouv√© course (alt): {hippodrome} - {full_url}")
                            links_courses.append({"url": full_url, "hippodrome": hippodrome})
            
            print(f"‚úÖ Trouv√© {len(links_courses)} courses de {filtre_type}")
            return links_courses
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche des courses: {str(e)}")
            traceback.print_exc()
            return []
    
    def extract_course_details(self, driver, course_url, hippodrome):
        """Extrait les d√©tails de toutes les courses d'un hippodrome"""
        print(f"üìç Scraping des courses √† {hippodrome}...")
        
        driver.get(course_url)
        time.sleep(5)  # Augmentation du d√©lai
        
        courses_data = {
            "hippodrome": hippodrome,
            "date_extraction": datetime.now().isoformat(),
            "url_source": course_url,
            "courses": []
        }
        
        try:
            # Prenons une capture d'√©cran pour le d√©bogage
            screenshot_dir = os.path.join(self.output_dir, "debug")
            os.makedirs(screenshot_dir, exist_ok=True)
            screenshot_path = os.path.join(screenshot_dir, f"{hippodrome.lower().replace(' ', '_')}_reunion.png")
            driver.save_screenshot(screenshot_path)
            print(f"üì∏ Capture d'√©cran sauvegard√©e: {screenshot_path}")
            
            html = driver.page_source
            
            # Sauvegardons le HTML pour le d√©bogage
            with open(os.path.join(screenshot_dir, f"{hippodrome.lower().replace(' ', '_')}_reunion.html"), "w", encoding="utf-8") as f:
                f.write(html)
            
            soup = BeautifulSoup(html, "html.parser")
            
            # R√©cup√©rer la date de la r√©union
            date_element = soup.select_one(".event-date, .date, [class*='date']")
            if date_element:
                courses_data["date_reunion"] = date_element.text.strip()
                print(f"üìÖ Date de r√©union: {courses_data['date_reunion']}")
            else:
                print("‚ö†Ô∏è Date de r√©union non trouv√©e")
                # Essayons de r√©cup√©rer la date d'aujourd'hui
                courses_data["date_reunion"] = datetime.now().strftime("%d/%m/%Y")
                print(f"üìÖ Date de r√©union (par d√©faut): {courses_data['date_reunion']}")
            
            # R√©cup√©rer les informations de terrain
            terrain_element = soup.select_one(".field-terrain, .terrain, [class*='terrain']")
            if terrain_element:
                courses_data["terrain"] = terrain_element.text.strip()
                print(f"üå± Terrain: {courses_data['terrain']}")
            
            # R√©cup√©rer les liens vers chaque course (essayer plusieurs s√©lecteurs)
            course_links = soup.select("a[href*='/courses/fiche-course'], a[href*='fiche-course'], table a")
            print(f"üîó Trouv√© {len(course_links)} liens de courses")
            
            for index, link in enumerate(course_links):
                # V√©rifier si l'attribut href existe et s'il est valide
                href = link.get("href")
                if not href or href == "#" or not href.startswith(("/", "http")):
                    print(f"‚ö†Ô∏è Lien invalide trouv√©: {repr(href)}, ignor√©.")
                    continue
                
                course_name = link.text.strip()
                print(f"üîé Nom de course: {repr(course_name)}")
                
                # Ignorer les liens avec des noms vides ou suspects
                if not course_name or course_name == "-":
                    print("‚ö†Ô∏è Nom de course vide ou invalide, ignor√©.")
                    continue
                
                # S'assurer que c'est bien une course de plat
                parent_element = link.parent.parent if link.parent else None
                if parent_element:
                    parent_text = parent_element.get_text().strip()
                    # Si on trouve "Obstacle" dans le texte parent, c'est une course d'obstacles
                    if "Obstacle" in parent_text and "Plat" not in parent_text:
                        print(f"‚ö†Ô∏è Course d'obstacles d√©tect√©e, ignor√©e: {course_name}")
                        continue
                
                # Construire l'URL compl√®te
                course_url = f"{self.base_url}{href}" if href.startswith('/') else href
                
                print(f"  ‚è≥ Course {index+1}/{len(course_links)}: {course_name} - {course_url}")
                
                try:
                    # Aller sur la page de d√©tail de la course
                    driver.get(course_url)
                    time.sleep(3)
                    
                    # Prenons une capture d'√©cran pour le d√©bogage
                    screenshot_path = os.path.join(screenshot_dir, f"{hippodrome.lower().replace(' ', '_')}_course_{index+1}.png")
                    driver.save_screenshot(screenshot_path)
                    
                    course_html = driver.page_source
                    
                    # Sauvegardons le HTML pour le d√©bogage
                    with open(os.path.join(screenshot_dir, f"{hippodrome.lower().replace(' ', '_')}_course_{index+1}.html"), "w", encoding="utf-8") as f:
                        f.write(course_html)
                    
                    course_soup = BeautifulSoup(course_html, "html.parser")
                    
                    # V√©rifier une derni√®re fois si c'est une course de plat
                    page_text = course_soup.get_text().strip()
                    if "Obstacle" in page_text and "Plat" not in page_text:
                        print(f"‚ö†Ô∏è Page de course d'obstacles d√©tect√©e, ignor√©e: {course_name}")
                        continue
                    
                    # Extraire les d√©tails de la course
                    course_data = {
                        "nom": course_name,
                        "url": course_url,
                        "participants": []
                    }
                    
                    # Extraire les infos compl√©mentaires
                    infos = course_soup.select(".infos-complementaires li, .infos li, [class*='infos'] li")
                    for info in infos:
                        key_element = info.select_one("span.label, .label, strong, b")
                        value_element = info.select_one("span.value, .value")
                        
                        if key_element and value_element:
                            key = key_element.text.strip().rstrip(':')
                            value = value_element.text.strip()
                            course_data[key.lower().replace(' ', '_')] = value
                        elif key_element:
                            # Si on a seulement la cl√©, essayer d'extraire la valeur du reste du texte
                            info_text = info.get_text().strip()
                            key = key_element.text.strip().rstrip(':')
                            value = info_text.replace(key, '').strip(' :')
                            if value:
                                course_data[key.lower().replace(' ', '_')] = value
                    
                    # Horaire de la course (g√©n√©ralement en haut de la page)
                    horaire_element = course_soup.select_one(".horaire, .time, .heure, [class*='horaire'], [class*='time']")
                    if horaire_element:
                        course_data["horaire"] = horaire_element.text.strip()
                    
                    # PDF Programme
                    pdf_link = course_soup.select_one("a[href*='.pdf']")
                    if pdf_link and pdf_link.get('href'):
                        pdf_href = pdf_link.get('href')
                        course_data["pdf_programme"] = f"{self.base_url}{pdf_href}" if pdf_href.startswith('/') else pdf_href
                    
                    # Vid√©o replay
                    video_link = course_soup.select_one("a.video-link, a[href*='video'], [class*='video']")
                    if video_link and video_link.get('href'):
                        video_href = video_link.get('href')
                        course_data["video_replay"] = f"{self.base_url}{video_href}" if video_href.startswith('/') else video_href
                    
                    # Extraire les partants (participants)
                    table = course_soup.find("table", class_="tableaupartants") or course_soup.select_one("table")
                    
                    if table:
                        # Essayer de trouver les en-t√™tes dans le thead
                        thead = table.select_one("thead")
                        if thead:
                            headers = [th.text.strip() for th in thead.select("th")]
                        else:
                            # Sinon, utiliser la premi√®re ligne comme en-t√™tes
                            first_row = table.select_one("tr")
                            if first_row:
                                headers = [th.text.strip() for th in first_row.select("th")] or [td.text.strip() for td in first_row.select("td")]
                            else:
                                headers = []
                        
                        print(f"üìä En-t√™tes trouv√©s: {headers}")
                        
                        # S√©lectionner les lignes du corps du tableau
                        body_rows = table.select("tbody tr") if table.select_one("tbody") else table.select("tr")[1:] if table.select("tr") else []
                        
                        print(f"üìã Trouv√© {len(body_rows)} lignes de participants")
                        
                        for tr in body_rows:
                            cells = tr.find_all("td")
                            if len(cells) >= min(1, len(headers)):  # Au moins une cellule
                                participant = {}
                                
                                # Si nous avons des en-t√™tes, les utiliser pour nommer les colonnes
                                if headers:
                                    for i, header in enumerate(headers):
                                        if i < len(cells):  # √âviter l'index out of range
                                            key = header.lower().replace(' ', '_') if header else f"column_{i+1}"
                                            # R√©cup√©rer le texte et supprimer les espaces superflus
                                            value = cells[i].text.strip()
                                            participant[key] = value
                                            
                                            # Si c'est une cellule avec un lien (comme le nom du cheval)
                                            link = cells[i].find("a")
                                            if link and link.get("href"):
                                                url = f"{self.base_url}{link['href']}" if link['href'].startswith('/') else link['href']
                                                participant[f"{key}_url"] = url
                                else:
                                    # Sans en-t√™tes, utiliser des noms g√©n√©riques
                                    for i, cell in enumerate(cells):
                                        key = f"column_{i+1}"
                                        value = cell.text.strip()
                                        participant[key] = value
                                        
                                        # Chercher les liens
                                        link = cell.find("a")
                                        if link and link.get("href"):
                                            url = f"{self.base_url}{link['href']}" if link['href'].startswith('/') else link['href']
                                            participant[f"{key}_url"] = url
                                
                                course_data["participants"].append(participant)
                    
                    # MODIFICATION: Ne garder que les courses avec des participants r√©els
                    if course_data.get("participants") and len(course_data["participants"]) >= 2 and \
                       any(p.get("cheval") or p.get("n") or p.get("n¬∞") or p.get("jockey") or p.get("poids") for p in course_data["participants"]):
                        courses_data["courses"].append(course_data)
                        print(f"‚úÖ Course ajout√©e avec {len(course_data['participants'])} participants: {course_name}")
                    else:
                        print(f"‚ö†Ô∏è Course ignor√©e car trop peu de donn√©es: {course_name}")
                    
                except Exception as e:
                    print(f"‚ùå Erreur lors du traitement de la course {course_name}: {str(e)}")
                    traceback.print_exc()
                    # Continuer avec la course suivante malgr√© l'erreur
            
            # MODIFICATION: Marquer si le fichier est vide
            if not courses_data["courses"]:
                print(f"‚ö†Ô∏è Aucune course valide trouv√©e pour {hippodrome}, le fichier sera vide")
                courses_data["empty"] = True
                
            return courses_data
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction des courses √† {hippodrome}: {str(e)}")
            traceback.print_exc()
            courses_data["error"] = str(e)
            return courses_data
    
    def save_json(self, data, filename):
        """Sauvegarde les donn√©es au format JSON"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ Donn√©es sauvegard√©es dans {filepath}")
    
    def enrich_existing_json_files(self):
        """Parcourt tous les fichiers JSON de course existants pour les enrichir avec les d√©tails"""
        files = [f for f in os.listdir(self.output_dir) if f.endswith(".json")]
        
        print(f"üîÑ Enrichissement de {len(files)} fichiers JSON existants...")
        
        for filename in files:
            filepath = os.path.join(self.output_dir, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)

            url = data.get("url_source")
            hippodrome = data.get("hippodrome", filename.replace(".json", ""))
            
            if not url:
                print(f"‚ö†Ô∏è Pas d'URL source dans {filename}, fichier ignor√©.")
                continue
            
            print(f"üîç Re-scraping de {hippodrome} depuis {url}")
            driver = self.get_driver()
            try:
                enriched_data = self.extract_course_details(driver, url, hippodrome)
                self.save_json(enriched_data, filename)
            except Exception as e:
                print(f"‚ùå Erreur sur {filename}: {e}")
                traceback.print_exc()
            finally:
                driver.quit()
    
    def run(self, filtre_type="Plat", jours=1):
        """Ex√©cute le scraper complet"""
        print(f"üèá D√©but du scraping des courses de {filtre_type} pour les {jours} prochains jours")
        
        driver = self.get_driver()
        try:
            # R√©cup√©rer les liens des courses
            course_links = self.get_course_links(driver, filtre_type, jours)
            
            for i, course in enumerate(course_links):
                print(f"‚è≥ Traitement {i+1}/{len(course_links)}: {course['hippodrome']}")
                
                # Extraire les d√©tails des courses
                course_data = self.extract_course_details(driver, course["url"], course["hippodrome"])
                
                # G√©n√©rer un nom de fichier bas√© sur l'hippodrome et la date
                date_str = datetime.now().strftime("%Y-%m-%d")
                safe_name = course["hippodrome"].replace(" ", "_").replace("/", "-").lower()
                filename = f"{date_str}_{safe_name}.json"
                
                # Sauvegarder les donn√©es
                self.save_json(course_data, filename)
                
                # MODIFICATION: Supprimer les fichiers vides
                if not course_data.get("courses"):
                    filepath = os.path.join(self.output_dir, filename)
                    print(f"üóëÔ∏è Suppression du fichier JSON vide pour {course['hippodrome']}")
                    os.remove(filepath)
            
            print(f"üéâ Scraping termin√©! {len(course_links)} hippodromes trait√©s.")
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©rale: {str(e)}")
            traceback.print_exc()
        finally:
            driver.quit()
    
    def direct_scrape_url(self, url, filename=None):
        """Scrape directement une URL sp√©cifique de course"""
        print(f"üîç Scraping direct de l'URL: {url}")
        
        driver = self.get_driver()
        try:
            # D√©terminer le nom de l'hippodrome √† partir de l'URL ou utiliser un g√©n√©rique
            hippodrome = "course_directe"
            
            # Extraire les d√©tails des courses
            course_data = self.extract_course_details(driver, url, hippodrome)
            
            # Si un nom de fichier n'est pas fourni, en g√©n√©rer un
            if not filename:
                date_str = datetime.now().strftime("%Y-%m-%d")
                filename = f"{date_str}_direct_scrape.json"
            
            # Sauvegarder les donn√©es
            self.save_json(course_data, filename)
            
            # MODIFICATION: Supprimer si vide
            if not course_data.get("courses"):
                filepath = os.path.join(self.output_dir, filename)
                print(f"üóëÔ∏è Suppression du fichier JSON vide pour le scraping direct")
                os.remove(filepath)
            
            print(f"‚úÖ Scraping direct termin√© pour {url}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du scraping direct: {str(e)}")
            traceback.print_exc()
        finally:
            driver.quit()

if __name__ == "__main__":
    import os
    import sys
    
    # Obtenir les variables d'environnement (utile pour GitHub Actions)
    type_course = os.environ.get("TYPE_COURSE", "Plat")
    jours = int(os.environ.get("JOURS", "3"))
    mode = os.environ.get("MODE", "all")  # "all", "new", "enrich", "direct"
    direct_url = os.environ.get("URL", "")
    
    scraper = ScraperCoursesFG()
    
    # V√©rifier si on a des arguments en ligne de commande
    if len(sys.argv) > 1:
        # Si premier argument est une URL, faire un scraping direct
        if sys.argv[1].startswith("http"):
            direct_url = sys.argv[1]
            mode = "direct"
        # Sinon consid√©rer que c'est le mode
        else:
            mode = sys.argv[1]
    
    # Mode selon l'environnement ou valeur par d√©faut
    if mode == "direct" and direct_url:
        # Scraping direct d'une URL
        scraper.direct_scrape_url(direct_url)
    elif mode == "all" or mode == "new":
        # √âtape 1 : Scraper les nouvelles courses
        scraper.run(filtre_type=type_course, jours=jours)
    
    if mode == "all" or mode == "enrich":
        # √âtape 2 : Enrichir les JSON avec les d√©tails internes
        scraper.enrich_existing_json_files()
